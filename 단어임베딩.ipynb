{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "단어임베딩.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjn_jKMIlURB",
        "colab_type": "text"
      },
      "source": [
        "# 텍스트 벡터화 - 토큰 임베딩(단어 임베딩)\n",
        "\n",
        "\n",
        "*   텍스트 벡터화하는 방법으로 원-핫 인코딩과, 토큰 임베딩이 있다. \n",
        "*   이전에 원-핫 인코딩을 확인했으므로, 이번에는 토큰 임베딩에 대해 알아볼 것이다.\n",
        "*   **토큰 임베딩**은 일반적으로 단어에 대해서만 사용되므로 **단어 임베딩**이라고도 부른다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zvNChLep_3M",
        "colab_type": "text"
      },
      "source": [
        "![그림1](https://user-images.githubusercontent.com/65331451/88993253-58c5e580-d320-11ea-9455-cda913b67989.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azwmFIFir18n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   원-핫 단어 벡터: 대부분 0으로 채워져 있어서 희소(sparse)하고 고차원이다.\n",
        "\n",
        "  → 희소 + 고차원 + 수동 인코딩\n",
        "\n",
        "  → 단어가 많아지면 벡터 공간이 커지는데 비해, 실제 1인 값은 한 개 뿐이므로 매우 비효율적이다.\n",
        "\n",
        "*   단어 임베딩: 밀집하고 저차원의 실수형 벡터이고, 데이터로부터 학습된다.\n",
        "\n",
        "  → 밀집 + 저차원 + 데이터로부터 학습\n",
        "\n",
        "  → 원-핫 벡터와 달리, 단어의 특성이나 유사성도 나타낼 수 있다.\n",
        "\n",
        "\n",
        "⇒ 단어 임베딩을 사용하면 **더 적은 차원**에 **더 많은 정보**를 저장할 수 있게 된다.\n",
        "\n",
        "⇒ 임베딩 벡터는 초기에 랜덤값을 가지지만, 학습을 통해 값이 변경된다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xel5boAQpla7",
        "colab_type": "text"
      },
      "source": [
        "# Embedding Layer\n",
        "\n",
        "\n",
        "*   정수 인코딩 된 단어를 입력으로 받아서 임베딩을 수행한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GBUOke2sKtE",
        "colab_type": "text"
      },
      "source": [
        "     text=[['Hope','to','see','you','soon'],['Nice','to','see','you','again']]\n",
        "\n",
        "     # 각 단어에 대한 정수 인코딩\n",
        "     text=[[0,1,2,3,4],[5,1,2,3,6]]\n",
        "\n",
        "     # Embedding(단어 집합의 크기, 임베딩 후 얻는 벡터의 크기, 입력 시퀀스의 길이)\n",
        "     Embedding(7,2,input_length=5)\n",
        "\n",
        "→ ex) index 0 = [1.2, 3.1] 과 같이 2차원의 벡터를 갖게 된다. "
      ]
    }
  ]
}